# ğŸš€ éƒ¨ç½²èˆ‡ç”Ÿç”¢ç’°å¢ƒå»ºè­°

## âš ï¸ ç•¶å‰ç‰ˆæœ¬é™åˆ¶

ç›®å‰çš„å¯¦ä½œé©åˆ**é–‹ç™¼å’Œæ¸¬è©¦ç’°å¢ƒ**ï¼Œä½¿ç”¨äº†ä»¥ä¸‹ç°¡åŒ–æ–¹æ¡ˆï¼š

### ä»»å‹™ç®¡ç†
- âŒ ä½¿ç”¨å…§å­˜å­—å…¸å„²å­˜ä»»å‹™ç‹€æ…‹
- âŒ ä¼ºæœå™¨é‡å•Ÿå¾Œä»»å‹™ç‹€æ…‹æœƒéºå¤±
- âŒ ç„¡æ³•è·¨å¤šå€‹ worker å…±äº«ç‹€æ…‹

### èƒŒæ™¯ä»»å‹™
- âŒ ä½¿ç”¨ FastAPI BackgroundTasksï¼ˆå–®é€²ç¨‹ï¼‰
- âŒ ç„¡ä»»å‹™å„ªå…ˆç´š
- âŒ ç„¡ä»»å‹™é‡è©¦æ©Ÿåˆ¶
- âŒ ä¸¦ç™¼èƒ½åŠ›æœ‰é™

### å®‰å…¨æ€§
- âŒ ç„¡èº«ä»½é©—è­‰
- âŒ ç„¡ API é‡‘é‘°é©—è­‰
- âŒ ç„¡é€Ÿç‡é™åˆ¶
- âŒ CORS é–‹æ”¾éæ–¼å¯¬é¬†

### æª”æ¡ˆç®¡ç†
- âŒ æª”æ¡ˆå­˜æ”¾åœ¨æœ¬åœ°ç›®éŒ„
- âŒ ç„¡æª”æ¡ˆæ¸…ç†æ©Ÿåˆ¶
- âŒ ç„¡æª”æ¡ˆå¤§å°é™åˆ¶

---

## ğŸ—ï¸ ç”Ÿç”¢ç’°å¢ƒæ”¹é€²å»ºè­°

### 1. ä»»å‹™ç‹€æ…‹ç®¡ç† - ä½¿ç”¨ Redis

#### ç‚ºä»€éº¼éœ€è¦ Redis?
- âœ… æŒä¹…åŒ–å„²å­˜ï¼ˆä¼ºæœå™¨é‡å•Ÿå¾Œä»»å‹™ç‹€æ…‹ä¸éºå¤±ï¼‰
- âœ… è·¨å¤šå€‹ worker å…±äº«ç‹€æ…‹
- âœ… æ”¯æ´ TTLï¼ˆè‡ªå‹•éæœŸï¼‰
- âœ… é«˜æ•ˆèƒ½è®€å¯«

#### å¯¦ä½œç¯„ä¾‹

**å®‰è£ä¾è³´**:
```bash
pip install redis
```

**ä¿®æ”¹ `app/services/progress.py`**:
```python
import redis
import json
from typing import Dict, Any, Optional

class TaskProgress:
    """ä½¿ç”¨ Redis çš„ä»»å‹™é€²åº¦ç®¡ç†"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True
        )
        self.ttl = 86400  # ä»»å‹™ä¿ç•™ 24 å°æ™‚
    
    def create_task(self, user_prompt: str, email: str, **kwargs) -> str:
        task_id = str(uuid.uuid4())
        
        task_data = {
            "task_id": task_id,
            "status": "queued",
            "progress": 0,
            "user_prompt": user_prompt,
            "email": email,
            "created_at": datetime.now().isoformat(),
            **kwargs
        }
        
        # å„²å­˜åˆ° Redis
        self.redis_client.setex(
            f"task:{task_id}",
            self.ttl,
            json.dumps(task_data)
        )
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        data = self.redis_client.get(f"task:{task_id}")
        if data:
            return json.loads(data)
        return None
    
    def update_task(self, task_id: str, **kwargs):
        task = self.get_task(task_id)
        if task:
            task.update(kwargs)
            task["updated_at"] = datetime.now().isoformat()
            self.redis_client.setex(
                f"task:{task_id}",
                self.ttl,
                json.dumps(task)
            )
```

**Docker Compose é…ç½®**:
```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

---

### 2. èƒŒæ™¯ä»»å‹™ - ä½¿ç”¨ Celery

#### ç‚ºä»€éº¼éœ€è¦ Celery?
- âœ… åˆ†æ•£å¼ä»»å‹™ä½‡åˆ—
- âœ… æ”¯æ´ä»»å‹™å„ªå…ˆç´š
- âœ… æ”¯æ´ä»»å‹™é‡è©¦
- âœ… æ”¯æ´å®šæ™‚ä»»å‹™
- âœ… å¯æ“´å±•åˆ°å¤šå€‹ worker

#### å¯¦ä½œç¯„ä¾‹

**å®‰è£ä¾è³´**:
```bash
pip install celery[redis]
```

**å»ºç«‹ `app/celery_app.py`**:
```python
from celery import Celery
from config import Config

celery_app = Celery(
    'sea_news_alert',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=3600,  # 1 å°æ™‚è¶…æ™‚
)
```

**å»ºç«‹ `app/tasks.py`**:
```python
from app.celery_app import celery_app
from app.services.workflow import workflow

@celery_app.task(bind=True, max_retries=3)
def execute_news_report_task(self, task_id: str):
    """Celery ä»»å‹™: åŸ·è¡Œæ–°èå ±å‘Šç”Ÿæˆ"""
    try:
        # ä½¿ç”¨ async_to_sync å¦‚æœéœ€è¦
        workflow.execute_task(task_id)
    except Exception as e:
        # é‡è©¦æ©Ÿåˆ¶
        raise self.retry(exc=e, countdown=60)
```

**ä¿®æ”¹ `app/routers/tasks.py`**:
```python
from app.tasks import execute_news_report_task

@router.post("/news-report", response_model=TaskResponse, status_code=201)
async def create_news_report_task(request: NewsReportRequest):
    # å‰µå»ºä»»å‹™
    task_id = task_manager.create_task(...)
    
    # ä½¿ç”¨ Celery åŸ·è¡Œ
    execute_news_report_task.delay(task_id)
    
    return TaskResponse(task_id=task_id, message="Task started")
```

**å•Ÿå‹• Celery Worker**:
```bash
celery -A app.celery_app worker --loglevel=info
```

---

### 3. èº«ä»½é©—è­‰ - ä½¿ç”¨ JWT

#### å¯¦ä½œç¯„ä¾‹

**å®‰è£ä¾è³´**:
```bash
pip install python-jose[cryptography] passlib[bcrypt]
```

**å»ºç«‹ `app/auth.py`**:
```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from datetime import datetime, timedelta

security = HTTPBearer()

SECRET_KEY = "your-secret-key-here"  # æ‡‰å¾ç’°å¢ƒè®Šæ•¸è®€å–
ALGORITHM = "HS256"

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(hours=24)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    try:
        payload = jwt.decode(
            credentials.credentials,
            SECRET_KEY,
            algorithms=[ALGORITHM]
        )
        return payload
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )
```

**ä¿è­· API ç«¯é»**:
```python
from app.auth import verify_token

@router.post("/news-report", dependencies=[Depends(verify_token)])
async def create_news_report_task(request: NewsReportRequest):
    # ... åŸæœ‰é‚è¼¯
    pass
```

---

### 4. é€Ÿç‡é™åˆ¶

**å®‰è£ä¾è³´**:
```bash
pip install slowapi
```

**é…ç½®é€Ÿç‡é™åˆ¶**:
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@router.post("/news-report")
@limiter.limit("10/hour")  # æ¯å°æ™‚ 10 æ¬¡
async def create_news_report_task(request: Request, ...):
    pass
```

---

### 5. æª”æ¡ˆç®¡ç† - ä½¿ç”¨é›²ç«¯å„²å­˜

#### ä½¿ç”¨ AWS S3

**å®‰è£ä¾è³´**:
```bash
pip install boto3
```

**ä¸Šå‚³åˆ° S3**:
```python
import boto3
from pathlib import Path

s3_client = boto3.client(
    's3',
    aws_access_key_id='YOUR_KEY',
    aws_secret_access_key='YOUR_SECRET'
)

def upload_to_s3(file_path: Path, bucket_name: str):
    """ä¸Šå‚³æª”æ¡ˆåˆ° S3"""
    object_name = f"reports/{file_path.name}"
    
    s3_client.upload_file(
        str(file_path),
        bucket_name,
        object_name
    )
    
    # ç”Ÿæˆé ç°½å URLï¼ˆæœ‰æ•ˆæœŸ 1 å°æ™‚ï¼‰
    url = s3_client.generate_presigned_url(
        'get_object',
        Params={
            'Bucket': bucket_name,
            'Key': object_name
        },
        ExpiresIn=3600
    )
    
    return url
```

**ä¿®æ”¹ workflow**:
```python
# ç”Ÿæˆå ±å‘Šå¾Œ
pdf_path = self.report_agent.generate_pdf(markdown_report)
excel_path = self.report_agent.generate_excel(structured_news)

# ä¸Šå‚³åˆ° S3
pdf_url = upload_to_s3(pdf_path, 'your-bucket-name')
excel_url = upload_to_s3(excel_path, 'your-bucket-name')

# å„²å­˜ URL è€Œéæœ¬åœ°è·¯å¾‘
task_manager.set_succeeded(
    task_id,
    pdf_path=pdf_url,
    xlsx_path=excel_url
)

# åˆªé™¤æœ¬åœ°æª”æ¡ˆ
pdf_path.unlink()
excel_path.unlink()
```

---

### 6. æ—¥èªŒèˆ‡ç›£æ§

**ä½¿ç”¨çµæ§‹åŒ–æ—¥èªŒ**:
```python
import logging
import json

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
        }
        return json.dumps(log_data)

# è¨­å®š
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger("sea_news_alert")
logger.addHandler(handler)
logger.setLevel(logging.INFO)
```

**æ•´åˆç›£æ§å·¥å…·**:
```bash
# Prometheus
pip install prometheus-fastapi-instrumentator

# åœ¨ main.py ä¸­
from prometheus_fastapi_instrumentator import Instrumentator

Instrumentator().instrument(app).expose(app)
```

---

### 7. è³‡æ–™åº«æ•´åˆ

**ä½¿ç”¨ PostgreSQL**:
```bash
pip install sqlalchemy psycopg2-binary alembic
```

**å®šç¾©æ¨¡å‹**:
```python
from sqlalchemy import Column, String, Integer, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Task(Base):
    __tablename__ = 'tasks'
    
    id = Column(String, primary_key=True)
    status = Column(String, nullable=False)
    progress = Column(Integer, default=0)
    user_prompt = Column(Text)
    email = Column(String)
    error = Column(Text, nullable=True)
    pdf_path = Column(String, nullable=True)
    xlsx_path = Column(String, nullable=True)
    created_at = Column(DateTime)
    updated_at = Column(DateTime)
```

---

## ğŸ“¦ Docker éƒ¨ç½²

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£ä¾è³´
COPY requirements-api.txt .
RUN pip install --no-cache-dir -r requirements-api.txt

# è¤‡è£½ç¨‹å¼ç¢¼
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å•Ÿå‹•
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EMAIL_ADDRESS=${EMAIL_ADDRESS}
      - EMAIL_PASSWORD=${EMAIL_PASSWORD}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./reports:/app/reports

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  celery_worker:
    build: .
    command: celery -A app.celery_app worker --loglevel=info
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EMAIL_ADDRESS=${EMAIL_ADDRESS}
      - EMAIL_PASSWORD=${EMAIL_PASSWORD}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./reports:/app/reports

volumes:
  redis_data:
```

**å•Ÿå‹•**:
```bash
docker-compose up -d
```

---

## ğŸ” ç’°å¢ƒè®Šæ•¸ç®¡ç†

**ç”Ÿç”¢ç’°å¢ƒ `.env`**:
```env
# API
SECRET_KEY=your-secret-key-here
DEBUG=false
ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com

# OpenAI
OPENAI_API_KEY=sk-...

# Email
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_ADDRESS=your-email@gmail.com
EMAIL_PASSWORD=your-app-password

# Redis
REDIS_URL=redis://localhost:6379/0

# Database (å¦‚æœä½¿ç”¨)
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# AWS S3 (å¦‚æœä½¿ç”¨)
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
S3_BUCKET_NAME=your-bucket

# ç›£æ§
SENTRY_DSN=https://...
```

---

## ğŸŒ åå‘ä»£ç† - Nginx

**nginx.conf**:
```nginx
upstream fastapi {
    server localhost:8000;
}

server {
    listen 80;
    server_name yourdomain.com;

    # é‡å®šå‘åˆ° HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com;

    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    # å‰ç«¯éœæ…‹æ–‡ä»¶
    location /static {
        alias /path/to/public;
    }

    # API è«‹æ±‚
    location /api {
        proxy_pass http://fastapi;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # å…¶ä»–è«‹æ±‚
    location / {
        proxy_pass http://fastapi;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## ğŸ“Š æ€§èƒ½å„ªåŒ–å»ºè­°

### 1. ä½¿ç”¨é€£ç·šæ± 
```python
# Redis é€£ç·šæ± 
redis_pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    db=0,
    max_connections=50
)
redis_client = redis.Redis(connection_pool=redis_pool)
```

### 2. å¿«å–æ©Ÿåˆ¶
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_search_results(query: str):
    # å¿«å–æœå°‹çµæœ
    pass
```

### 3. éåŒæ­¥ I/O
```python
import asyncio
import aiohttp

async def async_search(query: str):
    async with aiohttp.ClientSession() as session:
        # éåŒæ­¥ HTTP è«‹æ±‚
        pass
```

---

## âœ… ç”Ÿç”¢ç’°å¢ƒæª¢æŸ¥æ¸…å–®

### åŸºç¤è¨­æ–½
- [ ] ä½¿ç”¨ Redis å„²å­˜ä»»å‹™ç‹€æ…‹
- [ ] ä½¿ç”¨ Celery è™•ç†èƒŒæ™¯ä»»å‹™
- [ ] ä½¿ç”¨ PostgreSQL/MySQL å„²å­˜è³‡æ–™
- [ ] ä½¿ç”¨ S3/é›²ç«¯å„²å­˜ç®¡ç†æª”æ¡ˆ
- [ ] è¨­å®š Nginx åå‘ä»£ç†
- [ ] å•Ÿç”¨ HTTPS (Let's Encrypt)

### å®‰å…¨æ€§
- [ ] å¯¦ä½œèº«ä»½é©—è­‰ (JWT/OAuth)
- [ ] å•Ÿç”¨ API é‡‘é‘°é©—è­‰
- [ ] è¨­å®šé€Ÿç‡é™åˆ¶
- [ ] é™åˆ¶ CORS ä¾†æº
- [ ] éš±è—éŒ¯èª¤è©³æƒ…ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
- [ ] å®šæœŸæ›´æ–°ä¾è³´å¥—ä»¶

### ç›£æ§èˆ‡æ—¥èªŒ
- [ ] è¨­å®šçµæ§‹åŒ–æ—¥èªŒ
- [ ] æ•´åˆéŒ¯èª¤è¿½è¹¤ (Sentry)
- [ ] è¨­å®š Prometheus/Grafana ç›£æ§
- [ ] è¨­å®šå¥åº·æª¢æŸ¥ç«¯é»
- [ ] è¨­å®šå‘Šè­¦æ©Ÿåˆ¶

### å‚™ä»½èˆ‡ç½é›£æ¢å¾©
- [ ] è³‡æ–™åº«å®šæœŸå‚™ä»½
- [ ] Redis æŒä¹…åŒ–è¨­å®š
- [ ] æª”æ¡ˆå‚™ä»½ç­–ç•¥
- [ ] ç½é›£æ¢å¾©è¨ˆç•«

### æ•ˆèƒ½
- [ ] å•Ÿç”¨å¿«å–æ©Ÿåˆ¶
- [ ] ä½¿ç”¨é€£ç·šæ± 
- [ ] è¨­å®šé©ç•¶çš„è¶…æ™‚æ™‚é–“
- [ ] å£“ç¸® API éŸ¿æ‡‰ (gzip)
- [ ] CDN è¨­å®šï¼ˆéœæ…‹æ–‡ä»¶ï¼‰

---

**æ–‡ä»¶ç‰ˆæœ¬**: 1.0  
**æœ€å¾Œæ›´æ–°**: 2025-01-16
