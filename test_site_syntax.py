"""
æ¸¬è©¦ DuckDuckGo site: èªžæ³•çš„æ–°èžä¾†æºé™åˆ¶åŠŸèƒ½
é©—è­‰ AI èƒ½å¦æ­£ç¢ºä½¿ç”¨ site: èªžæ³•æœå°‹æŒ‡å®šä¾†æº
"""
import sys
from agents.research_agent import ResearchAgent
from urllib.parse import urlparse
import re


def extract_domain(url):
    """å¾ž URL æå–åŸŸå"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        # ç§»é™¤ www. å‰ç¶´
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except:
        return None


def test_site_syntax_search():
    """æ¸¬è©¦ä½¿ç”¨ site: èªžæ³•çš„æœå°‹åŠŸèƒ½"""
    print("="*70)
    print("æ¸¬è©¦ DuckDuckGo site: èªžæ³• - æ–°èžä¾†æºé™åˆ¶")
    print("="*70)
    
    # åˆå§‹åŒ– Research Agent
    agent = ResearchAgent()
    
    # ç²å–å…è¨±çš„åŸŸååˆ—è¡¨
    allowed_domains = [src['domain'].lower() for src in agent.TRUSTED_NEWS_SOURCES]
    print(f"\nðŸ“‹ å…è¨±çš„18å€‹æ–°èžä¾†æºåŸŸå:")
    for i, domain in enumerate(allowed_domains, 1):
        src = agent.TRUSTED_NEWS_SOURCES[i-1]
        print(f"   {i}. {src['name']} ({domain}) - {src['region']}")
    
    # æ¸¬è©¦æŸ¥è©¢
    test_query = "æ³°åœ‹æ•¸ä½æ”¯ä»˜"
    print(f"\nðŸ” æ¸¬è©¦æŸ¥è©¢: {test_query}")
    print(f"   æ™‚é–“ç¯„åœ: æœ€è¿‘ 30 å¤©å…§")
    print(f"   æ•¸é‡è¦æ±‚: 5-8ç¯‡")
    print(f"   èªžè¨€åå¥½: English")
    print("\nâ³ æ­£åœ¨æœå°‹ï¼ˆAI æœƒä½¿ç”¨ site: èªžæ³•é™åˆ¶ä¾†æºï¼‰...\n")
    
    # åŸ·è¡Œæœå°‹
    result = agent.search(
        query=test_query,
        time_instruction="æœ€è¿‘ 30 å¤©å…§",
        num_instruction="5-8ç¯‡",
        language="English"
    )
    
    if result['status'] != 'success':
        print(f"âŒ æœå°‹å¤±æ•—: {result.get('error', 'Unknown error')}")
        return
    
    # åˆ†æžçµæžœ
    content = result['content']
    print("\n" + "="*70)
    print("ðŸ“Š æœå°‹çµæžœåˆ†æž")
    print("="*70)
    
    # å˜—è©¦æå– JSON çµæžœ
    try:
        # å°‹æ‰¾ JSON å€å¡Š
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
        if json_match:
            import json
            data = json.loads(json_match.group(1))
            results = data.get('results', [])
            
            if len(results) == 0:
                print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–°èž")
                print("\né€™å¯èƒ½è¡¨ç¤ºï¼š")
                print("1. æŒ‡å®šç¶²ç«™ç¢ºå¯¦æ²’æœ‰ç›¸é—œæ–°èž")
                print("2. DuckDuckGo å°é€™äº›ç¶²ç«™çš„ç´¢å¼•è¦†è“‹åº¦æœ‰é™")
                print("3. site: èªžæ³•é™åˆ¶éŽæ–¼åš´æ ¼")
                print("\nå»ºè­°ï¼šå˜—è©¦æ›´å»£æ³›çš„æœå°‹é—œéµå­—æˆ–æ“´å¤§æ™‚é–“ç¯„åœ")
                return
            
            print(f"\nâœ… æ‰¾åˆ° {len(results)} ç¯‡æ–°èž")
            
            # é©—è­‰æ¯æ¢æ–°èžçš„ä¾†æº
            valid_count = 0
            invalid_count = 0
            invalid_sources = []
            
            for i, news in enumerate(results, 1):
                url = news.get('url', '')
                title = news.get('title', 'N/A')
                source = news.get('source', 'N/A')
                
                domain = extract_domain(url)
                is_valid = domain in allowed_domains
                
                status = "âœ…" if is_valid else "âŒ"
                print(f"\n{i}. {status} {title[:60]}...")
                print(f"   ä¾†æº: {source}")
                print(f"   URL: {url}")
                print(f"   åŸŸå: {domain}")
                
                if is_valid:
                    valid_count += 1
                else:
                    invalid_count += 1
                    invalid_sources.append((domain, url))
            
            # çµ±è¨ˆçµæžœ
            print("\n" + "="*70)
            print("ðŸ“ˆ é©—è­‰çµ±è¨ˆ")
            print("="*70)
            print(f"âœ… ç¬¦åˆæŒ‡å®šä¾†æº: {valid_count} ç¯‡ ({valid_count/len(results)*100:.1f}%)")
            print(f"âŒ éžæŒ‡å®šä¾†æº: {invalid_count} ç¯‡ ({invalid_count/len(results)*100:.1f}%)")
            
            if invalid_count > 0:
                print(f"\nâš ï¸ ç™¼ç¾ {invalid_count} å€‹éžæŒ‡å®šä¾†æº:")
                for domain, url in invalid_sources:
                    print(f"   - {domain}: {url}")
                print("\nðŸ’¡ å»ºè­°: éœ€è¦åœ¨ prompt ä¸­æ›´å¼·èª¿ site: èªžæ³•çš„ä½¿ç”¨")
            else:
                print(f"\nðŸŽ‰ å®Œç¾Žï¼æ‰€æœ‰æ–°èžéƒ½ä¾†è‡ªæŒ‡å®šçš„18å€‹å¯ä¿¡ç¶²ç«™")
                print("âœ… site: èªžæ³•é™åˆ¶ç­–ç•¥æˆåŠŸï¼")
            
            # ä¾†æºåˆ†å¸ƒçµ±è¨ˆ
            source_distribution = {}
            for news in results:
                domain = extract_domain(news.get('url', ''))
                if domain in allowed_domains:
                    source_distribution[domain] = source_distribution.get(domain, 0) + 1
            
            if source_distribution:
                print(f"\nðŸ“Š ä¾†æºåˆ†å¸ƒ:")
                for domain, count in sorted(source_distribution.items(), key=lambda x: x[1], reverse=True):
                    # æ‰¾åˆ°å°æ‡‰çš„ç¶²ç«™åç¨±
                    site_name = next((src['name'] for src in agent.TRUSTED_NEWS_SOURCES if src['domain'].lower() == domain), domain)
                    print(f"   {site_name} ({domain}): {count} ç¯‡")
            
        else:
            print("âš ï¸ ç„¡æ³•å¾žå›žæ‡‰ä¸­æå– JSON æ ¼å¼çš„çµæžœ")
            print("\nå®Œæ•´å›žæ‡‰:")
            print(content[:1000] + "..." if len(content) > 1000 else content)
    
    except Exception as e:
        print(f"âŒ è§£æžçµæžœæ™‚å‡ºéŒ¯: {str(e)}")
        print("\nå®Œæ•´å›žæ‡‰:")
        print(content[:1000] + "..." if len(content) > 1000 else content)


if __name__ == "__main__":
    test_site_syntax_search()
